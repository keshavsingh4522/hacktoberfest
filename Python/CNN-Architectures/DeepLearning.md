# Deep-Learning
<h2>SuperHero of Neural-network Era..</h2>

![Image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS2eb9z5PvCrgYe9ZS7sKMk6JGBA3cRqtW62Q&usqp=CAU)

# SO you might be thinking what is **DeepLeaning** and how it evolved over-time?

_Here is your answer in this article where you get the insight of what is deep-learning and some history of deep learning and what is it and I will keep this presice and as simple as possible to understand._

## SO what is Deep_learning?
Deep learning is a class of machine learning algorithms
that uses multiple layers to progressively extract higher-level
features from the raw input. The raw input can be text, image, or anything.
For example, in image processing, lower layers may identify edges, while higher layers
may identify edges, while higher layers may identify the concepts 
relevant to a human such as digits or letters or faces.

#### But where do it stands in machine-learning set?

![Image](https://qph.fs.quoracdn.net/main-qimg-a5a62840b9eff7e010c49a6f3fec13e5)


## Nueral networks: a brief history

In 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, teamed up to develop a mathematical model of an artificial neuron.They declared that:

- Beacause of the "all-or-none" characters of neural activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms. 


![Image](https://www.ee.co.za/wp-content/uploads/2019/07/Application-of-machine-learning-algorithms-in-boiler-plant-root-cause-analysis-Fig-1.jpg)


Here comes the model which resembles just like the boilogical neuron :

- where Neuron inputs == inputs from other neurons through dendrites
- Input weights == how well the dendrides connected to nucleus 
- Summing function == which is the main cell body of the neurons that sums up every inputs function with respects to its weight 
- Activation function == Axon is activation or firing part of the neuron that fires the required organ that needs to respond (i.e. the neurons which needs to be activated) and gives us the final result.
- output == Which is generated by summing up the fired neurons and passes out as result to next neuron , which is synapse which connects other neuron in body.

## First Machine Model

**We are about to witness the birth of such a machine - a machine capable of perceiving recognizing and identifying its surroundings without any human training or control.**
> by Frank Rosenblatt 

Here comes the invention of perceptron in 1958 at the <span style="color:blue">cornell Aeronautical Laboratory</span> by <span style="color:blue">Frank Rosenblatt</span>, funded by the united states Office of Naval Research.

### Perceptron

Mark I Perceptron machine, the first implementation of the perceptron algorithm. It was connected to a camera with 20x20 cadmium sulfide photocells to make a 400-pixel image. The main visible feature is a patch panel that set different combinations of input features. To the right, arrays of potentiometers that implemented the adaptive weights.

![Image](https://upload.wikimedia.org/wikipedia/en/thumb/5/52/Mark_I_perceptron.jpeg/220px-Mark_I_perceptron.jpeg)

That's how the complex the connections are but it's the computation that made us ease of doing those things right now.

## Theory of mis-interpretation 

Then comes the theory against the perceptron learning by an MIT professor named __Marvin Minsky__(Who was a grade behind Rosenblatt at the same high school) along with __Seymour Papert__ wrote a book, called "Perceptrons", about the Rosenblatt's invention. They showed that a single layer of these devices was unable to learn some simple, critical mathematical functions(such as XOR).In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed.

Unfortunately, the researchers only addressed the insights of Perceptron and entirely gave up on neural networks for about two decades.

## Parallel Distributed Processing(PDP), released in 1986:

`By J.L. McCLELLAND, D.E. RUMELHART, and G.E. HINTON`

![Image](/home/venkatesh/Downloads/pdp.png)

One of the excellent theory by the professor Marvin. If you keenly observe the process that's what we are currently adapting to train any neural network.

## The age of deep learning is here 

Then comes the age of deep learning in the 1980's where most models were built with a second layer of neurons, thus avoiding the problem
that had been identified by Minsky. However, again a misunderstanding of the 
theoretical issues held back the field. In theory, adding just one extra layer of neurons
was enough to allow any mathematical model to be approximated
with these neural networks, but in practice such networks
were often too big and slow to be useful.

**Researchers showed 30years ago that to get practical good
performance you need to use even more layers of neurons.**

## Timeline 

> Here is where we stand as of now: 

![Image](https://maelfabien.github.io/assets/images/ros_1.jpg)

### Final Note 
- > This is my first medium article hope so you get some
  > insight about deep learning. Thanks for reading for 
  > more info like that please share and subscribe.