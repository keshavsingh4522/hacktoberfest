{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_tree_implementation_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6vYUkEoSB0"
      },
      "source": [
        "\n",
        "# DECISION TREE FROM SCRATCH [CLASSIFICATION USING ENTROPY]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoM-vjn48O8u"
      },
      "source": [
        "#import the necessary libraries \n",
        "import pandas as pd \n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwXCiJnj8pHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5eb7b4b-3006-47b6-8bac-0aafaadd8ed1"
      },
      "source": [
        "data=pd.read_csv(\"zoo.csv\")\n",
        "data.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['animal_name', 'hair', 'feathers', 'eggs', 'milk', 'airborne',\n",
              "       'aquatic', 'predator', 'toothed', 'backbone', 'breathes', 'venomous',\n",
              "       'fins', 'legs', 'tail', 'domestic', 'catsize', 'class_type'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y3NowhCHejS"
      },
      "source": [
        "features=['hair', 'feathers', 'eggs', 'milk', 'airborne',\n",
        "       'aquatic', 'predator', 'toothed', 'backbone', 'breathes', 'venomous',\n",
        "       'fins', 'legs', 'tail', 'domestic', 'catsize']\n",
        "target=\"class_type\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL6sFYy2f_-N"
      },
      "source": [
        "data.drop(columns=\"animal_name\",axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlYQGF8bB2se"
      },
      "source": [
        "##ENTROPY\n",
        "\n",
        "\n",
        "$ entropy(x) = -\\sum_{for \\ vals \\ \\in column}(P(x=vals)*log_2(P(x=vals)))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpaORsuB8zua"
      },
      "source": [
        "def calculate_entropy(column):\n",
        "  \"\"\"\n",
        "  Parameters: \n",
        "      column : the column whose entropy has to be found out.\n",
        "  About the   Function:\n",
        "      Using entropy formula the entropy of the column is calculated\n",
        "  Return Type:\n",
        "      Floating point no.\n",
        "  Called In:\n",
        "      information_gain()\n",
        "  \"\"\"\n",
        "  entropy =0.0\n",
        "  vals,count = np.unique(column,return_counts=True) \n",
        "  # vals: the different values in the column\n",
        "  # count : the frequency of the different values in that column\n",
        "  \n",
        "  for i in range(len(vals)):\n",
        "    #the entropy formula is applied all the possible vals of the column and ultimately theies individual entropies will be added\n",
        "    p=count[i]/np.sum(count)\n",
        "    entropy=entropy+(-p*(np.log2(p)) )\n",
        "\n",
        "  return entropy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swCtnVgonoHE"
      },
      "source": [
        "##INFORMATION GAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69VFYh6IB1AA"
      },
      "source": [
        "def information_gain(data, split_decision_feature, target_col=\"target\"):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "      data          = the data_frame on which the information gain has to be calculated.\n",
        "      split_feature = the feature based on which the decision is to be made.\n",
        "      target _col   =  required to find the weighted entropy of the split_feature \n",
        "  About the Function \n",
        "      It takes in the parameter and finds out the information gained  \n",
        "      *formala* [  information_gain = total_entropy  -   weighted_entropy    ]\n",
        "      total_entropy = entropy of the data_frame \n",
        "  Return type:\n",
        "       information_gain float type\n",
        "  Called in:\n",
        "        ID3()\n",
        "  \"\"\"\n",
        "  weighted_entropy = 0.0\n",
        "\n",
        "  #Calculate total_entropy\n",
        "  total_entropy = calculate_entropy( data[target_col] )\n",
        "\n",
        "  #Calculate value and  frequency of each value in the split_feature column\n",
        "  vals,counts  = np.unique( data[split_decision_feature] , return_counts=True )\n",
        "\n",
        "  #based each different vals calculation of weighted entropy\n",
        "  for i in range(len(vals)):\n",
        "\n",
        "    #weight: It is the probability that each vals of the split_decision_feature occurs. \n",
        "    weight = counts[i]/( np.sum(counts) )\n",
        "   \n",
        "    ##################################################################################################################################################################################################################\n",
        "    #  p_w : the entropy of each vals of the split decision feature.                                                                                                                                                 #\n",
        "    #  function_description:                                                                                                                                                                                         #\n",
        "    #     ->Here we are calling entropy function to calculated p_w.                                                                                                                                                  #\n",
        "    #     ->data.where function is used to return a dataframe which tells us where in the dataset which gives us the idea of places where the particular value in vals exist                                         #\n",
        "    #         suppose there is a column in a data that we are considering is 'Weather' which has vals of 'sunny','rainy' and 'cold'                                                                                  #\n",
        "    #         so what data.where(data.weather==vals[\"sunny\"]) would do is it'll return a datset which tells us where in the dataset the value of the weather column is sunny.                                        #\n",
        "    #         suppose  the target_feature was \"go out an play\"  which has vals of \"yes,\"no\"                                                                                                                          #\n",
        "    #         np.where(data.weather==vals[\"sunny\"]).dropna() would drop all the instances in the dataset which has null value                                                                                        #\n",
        "    #         np.where(data.weather==vals[\"sunny\"]).dropna().target_feature this function would return the instances in the target_feature column where corresponding to the weather feature the vals vwas sunny     #\n",
        "    #         so this function returns the entropy of  the above case                                                                                                                                                #\n",
        "    ##################################################################################################################################################################################################################     \n",
        "    p_w = calculate_entropy(data.where(data[split_decision_feature] == vals[i]).dropna()[target_col])\n",
        "\n",
        "    #calculate the weighted_entropy\n",
        "    weighted_entropy = weighted_entropy+ p_w*weight\n",
        "    \n",
        "  #Calculate information gain\n",
        "  info_gain =total_entropy-weighted_entropy\n",
        "\n",
        "  \n",
        "  return info_gain\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Sj5h3uEtVS",
        "outputId": "72cdf090-abd3-49f8-b670-de1c677d2525"
      },
      "source": [
        "information_gain(data,\"toothed\",target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8656941534932372"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jVrm60in7bx"
      },
      "source": [
        "##ID3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwUpT3Wy__nu"
      },
      "source": [
        "def ID3(data,original_data,features,target_col=target,parent_mode_class=None):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "\n",
        "    data          : It is the dataframe that keeps on changing as the tree grows. It splits accorrding to decision node depending upon the \n",
        "                    unique values in the target column\n",
        "\n",
        "    original_data : We need to refer to the original data as and when the requirement arises in the case we need to stop the tree and so on.\n",
        "\n",
        "    features      : Keeps the track of features that remain after a decision node is reached.\n",
        "                    At the decision node the feature having highest information gain is ruled out\n",
        "\n",
        "    target_col   : The column based on which decision has to mbe made that is branching is done based on this node\n",
        "\n",
        "  Return Type:  This is a recursive call \n",
        "\n",
        "  Called In: ID3 \n",
        "\n",
        "  About the Function:\n",
        "      Using the information_gain function we would be recursively finding the column that is having the largest information \n",
        "      gain in each level of the decision tree. This is an recursive function it must have a stopping criteria which will alow\n",
        "      it to reach its leaf node where probably no more decision can be taken.\n",
        "\n",
        "      We stop the tree from growing(stopping criteria) when:\n",
        "      1. All rows in the target feature have the same value(only one decision can be taken no matter what is there in the feature column)\n",
        "      2. The dataset can be no longer split since there are no more features left\n",
        "      3. The dataset can no longer be split since there are no more rows left / There is no data left\n",
        "\n",
        "      By definition, we say that if the growing gets stopped because of stopping criteria two, the leaf node should predict\n",
        "      the most frequently occurring target feature value of the superior (parent) node. If the growing gets stopped because of the \n",
        "      third stopping criteria, we assign the leaf node the mode target feature value of the original dataset.\n",
        "  \"\"\"\n",
        "     ##################################################################################################################\n",
        "      #1. defining the stopping criteria\n",
        "\n",
        "      #   1a. All rows(elements) in the target column have the same value\n",
        "  if len(np.unique(data[target_col]))<=1:\n",
        "    return ( np.unique(data[target_col])[0])\n",
        "\n",
        "      #  1b. The dataset can be no longer split since there are no more features left\n",
        "      #        Note that:\n",
        "      #        If the feature space is empty, return the mode target feature value of the direct parent node \n",
        "      #        the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "      #        the mode target feature value is stored in the parent_node_class variable.\n",
        "  if len(features) == 0:\n",
        "    return (parent_mode_class)\n",
        "\n",
        "      #   1c.The dataset can no longer be split since there are no more rows left / There is no data left \n",
        "      #      return the mode in the target column if this is the case\n",
        "  if len(data) == 0:\n",
        "    return (original_data[target_col].mode()[0])\n",
        "\n",
        "      #################################################################################################################\n",
        "      # end of stopping  criterias!!!!! lets grow the treee\n",
        "\n",
        "      # set the parent_node_class  as  the mode of the target_col\n",
        "      # this variable must hold information about the mode of the target column at the current decision node \n",
        "  parent_mode_class = data[target_col].mode()[0]\n",
        "\n",
        "      #Lets claculate the information gain for each feature and see who has the highest information_gain\n",
        "  feature_info_gain =[]\n",
        "\n",
        "  for feature in features:\n",
        "       #storing the info gain of every feature present in the sub data\n",
        "    feature_info_gain.append(information_gain(data,feature,target_col))\n",
        "\n",
        "       # finding the index of the feature having highest info_gain value \n",
        "  feature_index = np.argmax(feature_info_gain)\n",
        "\n",
        "       # best feature = feature having highest info_gain value \n",
        "  best_feature=features[feature_index]\n",
        "\n",
        "  tree = {best_feature:{}}\n",
        "\n",
        "       #Updating  features list by eliminating the best_feature\n",
        "  features = [i for i in features if i != best_feature]\n",
        "\n",
        "       # lets start growing the branch for different values of best_feature\n",
        "  for value in np.unique(data[best_feature]):\n",
        "  \n",
        "    \n",
        "    sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "    subtree = ID3(sub_data,original_data,features,target_col,parent_mode_class)\n",
        "             \n",
        "    #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            #every best_feature will have value \n",
        "    tree[best_feature][value] = subtree\n",
        "            \n",
        "  return (tree)  \n",
        "  \n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoJHpkvioFBN"
      },
      "source": [
        "##PREDICT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJGfFAapHgt2"
      },
      "source": [
        "def predict(query,tree,default=1):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "      query:  represents one row in a data set. This query is of dict type. \n",
        "              keys are the column name and values are the value of in the \n",
        "              column for that particular row.\n",
        "\n",
        "      tree :  this particular parameter keeps on changing wrt the recursive calls are made. \n",
        "              This tree is prepared using the train data. As we move down the root node the \n",
        "              subsequent key values are removed starting with root node.\n",
        "\n",
        "      default=1 helps in coming out of a situation when the key is currenty not\n",
        "               in the list of keys in tree dict.\n",
        "\n",
        "\n",
        "\n",
        "   About The Function:\n",
        "               Now that we are done trainning some part of the data which resulted in growing a tree, \n",
        "               we need a function to make predictions on the test data.\n",
        "               row by row we predict the target column usying the tree that is obtained.\n",
        "               Procedure:\n",
        "               --for every key in the query first check if the key in the query is present as the root node of the tree\n",
        "                if it is the case then store result as the value of the tree's key that is root node.\n",
        "                Except block will take care if the current key of the query is not present in tree's keys by retuerning default.\n",
        "                After root nodes value is stored in the result then tree will have a new key that will be a interior node.\n",
        "                This process keeps on repeating until leaf node is reached, to determine this situation we can check if the result is  not dictionary \n",
        "                if it is still a dictionary then we go for recursion of the function untoil leaf node is reached .\n",
        "                what we actually do is we check for the particular values of the test query that is the corresponding leaf node or the decision in the tree constructed.\n",
        "                suppose\n",
        "                query={'breathes': 'True', 'legs': 'True', 'toothed': 'True'}\n",
        "                tree =     {'legs': {'False': 'Reptile','True': {'toothed': {'False': {'breathes': {'True': 'Mammal'}},'True': 'Mammal'}}}}\n",
        "                initially,\n",
        "                \n",
        "                list(query.keys() )   = ['breathes', 'legs', 'toothed']\n",
        "                list(tree.keys()  )   = 'legs'\n",
        "                when the loop reaches the index of legs for query then \n",
        "                key='legs'\n",
        "                query['leg'] which is 'True'\n",
        "\n",
        "                result = tree[key][query[key]] \n",
        "                result = tree[\"legs\"][query[\"leg\"]]\n",
        "                       =  tree[\"legs\"][\"True\"]\n",
        "                         {'toothed': {'False': {'breathes': {'True': 'Mammal'}},'True': 'Mammal'}}\n",
        "                so we will move towards the decision \"True\" for root node \"legs\" in tree.\n",
        "               \n",
        "\n",
        "\n",
        "                list(query.keys() )   = ['breathes', 'legs', 'toothed']\n",
        "                list(tree.keys()  )   ='toothed'\n",
        "                when the loop reaches the index of 'toothed' for query then \n",
        "                key='toothed'\n",
        "                query['toothed']=\"True\"\n",
        "                result= tree[\"toothed\"][query[\"toothed\"]]\n",
        "                        =tree[\"toothed\"][\"True\"]\n",
        "                        =\"Mammal\"\n",
        "\n",
        "                we reached the leaf node!!!! so mammal is the final decisioon           \n",
        "                \"\"\"\n",
        " #1.\n",
        "  for key in list(query.keys()):\n",
        "      if key in list(tree.keys()):\n",
        "            #2.\n",
        "          try:\n",
        "              result = tree[key][query[key]] \n",
        "              \n",
        "          except:\n",
        "              return default\n",
        "  \n",
        "            #3.\n",
        "          result = tree[key][query[key]]\n",
        "         \n",
        "\n",
        "            #4.\n",
        "          if isinstance(result,dict):\n",
        "\n",
        "            \n",
        "            return predict(query,result)\n",
        "\n",
        "          else:\n",
        "            return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyxR2oRwoDEq"
      },
      "source": [
        "LOADING DATA INTO THE FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnu6KADVfCOL",
        "outputId": "86a4b6e1-4312-48b9-de7c-1aaad7131dfb"
      },
      "source": [
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:80].reset_index(drop=True)#We drop the index respectively relabel the index\n",
        "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
        "    testing_data = dataset.iloc[80:].reset_index(drop=True)\n",
        "    return training_data,testing_data\n",
        "\n",
        "training_data = train_test_split(data)[0]\n",
        "testing_data = train_test_split(data)[1] \n",
        "\n",
        "\n",
        "\n",
        "def test(data,tree):\n",
        "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    #convert it to a dictionary\n",
        "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "    \n",
        "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
        "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class_type\"])/len(data))*100,'%')\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Train the tree, Print the tree and predict the accuracy\n",
        "\"\"\"\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
        "print(tree)\n",
        "test(testing_data,tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'legs': {0: {'fins': {0.0: {'toothed': {0.0: 7.0, 1.0: 3.0}}, 1.0: {'eggs': {0.0: 1.0, 1.0: 4.0}}}}, 2: {'hair': {0.0: 2.0, 1.0: 1.0}}, 4: {'hair': {0.0: {'toothed': {0.0: 7.0, 1.0: 5.0}}, 1.0: 1.0}}, 6: {'aquatic': {0.0: 6.0, 1.0: 7.0}}, 8: 7.0}}\n",
            "The prediction accuracy is:  85.71428571428571 %\n"
          ]
        }
      ]
    }
  ]
}